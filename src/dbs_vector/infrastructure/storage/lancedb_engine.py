import math
from typing import Any

import lancedb  # type: ignore[import-untyped]
import numpy as np
from numpy.typing import NDArray

from dbs_vector.core.ports import IStoreMapper


class LanceDBStore:
    """
    High-performance, Arrow-native storage backend using LanceDB.
    """

    def __init__(
        self,
        db_path: str,
        table_name: str,
        vector_dimension: int,
        mapper: IStoreMapper,
        nprobes: int = 20,
    ) -> None:
        self.db_path = db_path
        self.table_name = table_name
        self.vector_dimension = vector_dimension
        self.nprobes = nprobes
        self.mapper = mapper
        self.schema = mapper.schema

        self.db = lancedb.connect(db_path)
        # Use exist_ok to open existing tables rather than overwriting on app start
        self.table = self.db.create_table(self.table_name, schema=self.schema, exist_ok=True)

    def clear(self) -> None:
        """Drops the table and recreates it with an empty schema."""
        self.db.drop_table(self.table_name, ignore_missing=True)
        self.table = self.db.create_table(self.table_name, schema=self.schema)

    def ingest_chunks(self, chunks: list[Any], vectors: NDArray[np.float32]) -> None:
        """Constructs an Arrow RecordBatch to completely bypass Python iterators."""
        if not chunks:
            return

        arrow_batch = self.mapper.to_record_batch(chunks, vectors)

        self.table.add(arrow_batch)

    def compact(self) -> None:
        """Merges fragment Lance files generated by delta-updates."""
        self.table.optimize()

    def create_indices(self) -> None:
        """Generates dynamic IVF_PQ vector indices and Tantivy FTS indices."""
        total_rows = len(self.table)

        # 1. Vector Indexing (Cosine)
        if total_rows > 256:
            # Scale partitions dynamically
            num_partitions = max(1, min(256, int(math.sqrt(total_rows))))
            self.table.create_index(
                metric="cosine",
                vector_column_name="vector",
                index_type="IVF_PQ",
                num_partitions=num_partitions,
                num_sub_vectors=min(16, self.vector_dimension // 8),
            )

        # 2. Tantivy FTS Indexing (Hybrid Search)
        try:
            self.table.create_fts_index("text", replace=True)
        except Exception as e:
            print(f"Warning: FTS Indexing failed (tantivy missing?): {e}")

    def get_existing_hashes(self) -> set[str]:
        """Queries the table for all unique content hashes using Polars."""
        if len(self.table) == 0:
            return set()

        # Select just the content_hash column to minimize I/O
        df = self.table.to_polars(columns=["content_hash"])
        return set(df["content_hash"].unique().to_list())

    def search(
        self,
        query: str,
        query_vector: NDArray[np.float32],
        source_filter: str | None = None,
        limit: int = 5,
        **kwargs: Any,
    ) -> list[Any]:
        """Executes Hybrid (or pure Vector) search, filtering natively in Rust."""
        try:
            search_op = (
                self.table.search(query_type="hybrid")
                .vector(query_vector)
                .text(query)
                .nprobes(self.nprobes)
                .limit(limit)
            )
        except Exception as e:
            print(f"Hybrid search unavailable ({e}). Falling back to pure vector.")
            search_op = (
                self.table.search(query_vector).metric("cosine").nprobes(self.nprobes).limit(limit)
            )

        # Metadata Filtering (Rust-Level Pushdown)
        if source_filter:
            # SQL Injection Prevention
            safe_filter = source_filter.replace("'", "''")
            search_op = search_op.where(f"source = '{safe_filter}'", prefilter=True)

        # SQL specific pushdowns
        min_time = kwargs.get("min_time")
        if min_time is not None:
            search_op = search_op.where(f"execution_time_ms >= {min_time}", prefilter=True)

        results_df = search_op.to_polars()

        mapped_results: list[Any] = []
        for row in results_df.iter_rows(named=True):
            dist = row.get("_distance")
            score = float(dist) if isinstance(dist, float) else None
            mapped_results.append(self.mapper.from_polars_row(row, score))

        return mapped_results
